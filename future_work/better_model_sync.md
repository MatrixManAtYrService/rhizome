Currently we just capture the sql and generate models "by hand" (by LLM), but we can probably extend the existing `rhizome sync schema` mechanism to actually generate pydantic models.
If those models aren't to our liking for some reason we can wrap them much like we do for stolon.  Here's a writeup for the model creation part of that:

----

From SQL DDL to Pydantic Models: A Deterministic Workflow for sqlmodel GenerationI. Introduction: The Imperative for Deterministic Code GenerationIn modern software development, the velocity gained from automated code generation is indispensable. Toolchains that convert OpenAPI specifications into type-safe API clients have set a high standard for efficiency and reliability. The emergence of Large Language Models (LLMs) has introduced a powerful new paradigm for code generation, offering unprecedented flexibility in translating between formats, such as converting SQL table schemata into application-level data models. However, this flexibility comes at a cost. While LLMs excel at creative and one-off tasks, their probabilistic nature makes them fundamentally unsuited for generating foundational, mission-critical code that demands absolute precision and consistency.The Modern Developer's Dilemma: Speed vs. ReliabilityThe core challenge lies in a trade-off between the rapid scaffolding provided by LLMs and the rigorous reliability required for production systems. An LLM might generate a correct sqlmodel class from a CREATE TABLE statement 99 times out of 100, but the single failure—a mistyped field, an incorrect relationship, or a subtle deviation from best practices—can introduce bugs that are difficult to trace and costly to fix. This inherent non-determinism conflicts with the core principles of robust software engineering.Defining "Production-Grade" AutomationFor a code generation process to be considered "production-grade," it must satisfy three essential criteria:Determinism: The process must be repeatable and predictable. Given the same input SQL schema, it must produce the exact same output model code, every single time.Performance: The generation must be fast enough to integrate seamlessly into tight development feedback loops. A process that takes milliseconds can be run in a pre-commit hook; one that takes several seconds due to network latency is relegated to a manual, infrequent task.Introspectability: The logic of the generation tool should be transparent, configurable, and based on well-defined rules, not opaque, emergent behavior. Developers must be able to understand why a particular output was generated and how to influence it.Thesis Statement: The Two-Stage Reflection WorkflowThis report details a production-grade, non-LLM workflow for generating sqlmodel classes directly from SQL DDL. The solution is a two-stage, programmatic process that fully satisfies the criteria of determinism, performance, and introspectability. The architecture of this solution hinges on a crucial paradigm shift: instead of attempting the brittle and complex task of parsing raw SQL text, it leverages the mature and powerful mechanism of database reflection. The process involves:Materialization: The SQL DDL is executed against a transient, in-memory database, converting the static text into a live, structured schema.Reflection: A specialized tool connects to this ephemeral database, introspects its structure, and generates the corresponding sqlmodel code based on deterministic rules.This approach sidesteps the immense complexity of building a universal SQL parser—a task complicated by dozens of dialects and vendor-specific extensions. By first allowing a standard database engine (like SQLite) to interpret the DDL, the problem is transformed into a standardized one that the Python ecosystem, particularly SQLAlchemy, is exceptionally well-equipped to handle.1 This workflow provides the speed and automation of OpenAPI generators while ensuring the engineering rigor necessary for building reliable data layers.II. The Cornerstone Tool: A Deep Dive into sqlacodegenThe success of the reflection stage of the proposed workflow depends on a robust and specialized tool. The definitive choice for this task is sqlacodegen, a mature and widely used utility designed specifically to generate Python model code from live database schemas.1 It acts as the engine that translates the reflected database structure into clean, idiomatic, and ready-to-use sqlmodel classes.Core Functionality: From Live Schema to ORM Codesqlacodegen operates by connecting to a database using a SQLAlchemy engine URL. It then uses SQLAlchemy's powerful reflection capabilities to read the database's metadata—tables, columns, data types, constraints, and relationships. From this metadata, it generates Python code. Key features that make it ideal for this workflow include its support for modern SQLAlchemy 2.x, its ability to produce human-readable, PEP 8 compliant code, and its sophisticated logic for accurately detecting relationships between tables.1The sqlmodels Generator: The Direct SolutionWhile sqlacodegen has several output formats, its most relevant feature for this context is the sqlmodels generator. By specifying the --generator sqlmodels command-line flag, the tool is instructed to generate classes that inherit from sqlmodel.SQLModel.1 Since SQLModel is built directly on top of Pydantic and SQLAlchemy, this generator effectively produces Pydantic models that are also fully-featured SQLAlchemy ORM models, precisely matching the user's requirement.2 This purpose-built generator eliminates the code duplication and manual mapping that would otherwise be required when using separate Pydantic and SQLAlchemy models.4Mapping Semantics: How Relational Concepts are Translatedsqlacodegen employs a sophisticated set of rules to map relational database concepts into SQLModel code:Class Naming: Table names are converted to PEP 8 compliant, CamelCase class names. For example, a table named user_profiles becomes a class named UserProfiles.1Data Types: SQL data types are mapped to their corresponding Python types and SQLModel fields. VARCHAR becomes str, INTEGER becomes int, BOOLEAN becomes bool, and TIMESTAMP becomes datetime.datetime. Nullable columns (those without a NOT NULL constraint) are correctly typed as Optional[...].Constraints: Primary key and unique constraints are translated into arguments for Field(). A column defined as id INT PRIMARY KEY will be generated as id: Optional[int] = Field(default=None, primary_key=True).Relationship Detection: The tool automatically detects relationships based on foreign key constraints 1:Many-to-One: A standard foreign key constraint on a table generates a Relationship attribute pointing to the parent table.One-to-One: If a foreign key constraint also has a unique constraint on the same column(s), it is correctly identified as a one-to-one relationship.Relationship Naming: The naming logic is intuitive. A foreign key column named author_id in a posts table will generate a relationship attribute named author on the Post model. This intelligent naming, based on stripping the _id suffix, results in code that looks hand-written.1Critical Capabilities and Documented LimitationsWhile sqlacodegen is exceptionally powerful, it is essential to understand its boundaries. Its strength lies in automating the vast majority of the boilerplate involved in creating ORM models. For schemas involving simple one-to-one and many-to-one relationships, it often produces perfect, ready-to-use code.However, there is a significant documented limitation: the sqlmodel generator does not currently implement automatic detection of many-to-many relationships.3 This is not a failure of the tool but a specific feature gap that requires a planned, manual intervention. For a schema with a many-to-many relationship (e.g., posts and tags linked by a post_tags association table), the workflow should be:Run sqlacodegen on the entire schema. It will correctly generate three distinct models: Post, Tag, and PostTag.Manually edit the generated Post and Tag models.Add the link_model argument to the Relationship definitions, pointing to the PostTag model, and configure the back_populates attributes to complete the bidirectional relationship.This positions the tool correctly: it is a powerful accelerator that handles over 95% of the conversion work, leaving the developer to apply the final, nuanced touches for complex relationship patterns. This managed expectation is key to successfully integrating the tool into a professional workflow.To further customize the output, several generator-specific options are available. The most relevant options for the sqlmodels generator are summarized below.Table 1: Key sqlacodegen Generator Options for sqlmodelOptionDescription--generator sqlmodels(Required) Specifies that the output should be classes inheriting from sqlmodel.SQLModel.--options use_inflectUses the inflect library to singularize class names from plural table names (e.g., users table becomes User class). This is disabled by default to avoid incorrect inflections with non-English or complex nouns.1--options nojoinedDisables the automatic detection of joined-table inheritance. This can simplify the generated code if the database schema does not use this advanced ORM pattern.1--options nobidiGenerates unidirectional relationships only. The back_populates argument on Relationship will be omitted, which can be desirable for simpler API models or to avoid circular import issues.1III. The Crucial Bridge: Materializing DDL with In-Memory DatabasesThe primary challenge in automating the generation process is bridging the gap between the input format—a static SQL DDL file—and the input requirement of sqlacodegen, which is a live database connection URL.3 The most efficient and elegant solution to this impedance mismatch is to use an ephemeral, in-memory database to temporarily materialize the schema.The Impedance Mismatch: Static DDL vs. Dynamic ReflectionTools like sqlacodegen are built on the principle of dynamic reflection. They do not parse .sql text files. Instead, they leverage the mature, dialect-aware capabilities of SQLAlchemy's reflection system to query a database's internal schema catalogs (information_schema in PostgreSQL, sqlite_master in SQLite, etc.). This is a fundamentally more robust approach than text parsing. The problem, therefore, is how to transform a set of CREATE TABLE strings into a live database that SQLAlchemy can connect to and reflect.The Ephemeral Database Solution with SQLiteSQLite provides the perfect solution with its in-memory database mode. By using the special connection string :memory:, a new, fully functional SQLite database is created entirely within the application's memory space.6 This database is not written to disk and ceases to exist the moment the connection to it is closed.This approach offers several profound advantages for our workflow:Zero Dependencies: It requires no external database server, no Docker containers, and no complex setup. The sqlite3 module is part of the Python standard library.6Extreme Performance: All operations occur in RAM, making schema creation and reflection incredibly fast—typically completing in milliseconds. This is crucial for integration into rapid development cycles.Isolation: Each run creates a fresh, clean database, ensuring that the generation process is perfectly isolated and repeatable.7End-to-End Programmatic Workflow (The Core Recipe)The entire process—from reading DDL strings to generating SQLModel code—can be encapsulated in a single, self-contained Python script. This script serves as the core, reusable asset for the workflow.The following example demonstrates the complete, end-to-end process:Pythonimport io
import sqlite3
import sys

from sqlalchemy import create_engine, MetaData
from sqlacodegen.generators import SQLModelGenerator

# Step 1: Define the SQL DDL statements.
# This could be read from a.sql file in a real application.
sql_ddl = """
CREATE TABLE author (
    id INTEGER NOT NULL,
    name VARCHAR(255) NOT NULL,
    PRIMARY KEY (id)
);

CREATE TABLE book (
    id INTEGER NOT NULL,
    title VARCHAR(255) NOT NULL,
    author_id INTEGER NOT NULL,
    PRIMARY KEY (id),
    FOREIGN KEY(author_id) REFERENCES author (id)
);
"""

# Step 2: Create a SQLAlchemy engine connected to an in-memory SQLite database.
# The database exists only for the lifetime of the connection object.
engine = create_engine("sqlite:///:memory:")

# Step 3: Materialize the schema.
# Connect to the database and execute the DDL statements.
with engine.connect() as connection:
    # SQLite's Python driver doesn't support executing multiple statements
    # in a single string, so we split them.
    for statement in sql_ddl.strip().split(';'):
        if statement.strip():
            connection.exec_driver_sql(statement)

# Step 4: Programmatically invoke sqlacodegen for reflection and generation.
metadata = MetaData()

# Use SQLAlchemy's reflection to read the schema from the in-memory database.
metadata.reflect(bind=engine)

# Instantiate the SQLModel generator.
# The generator takes the reflected metadata as input.
generator = SQLModelGenerator(metadata)

# Use an in-memory text stream (StringIO) to capture the generated code.
# This avoids writing to a temporary file on disk.
outfile = io.StringIO()

# Render the generated code into the StringIO buffer.
generator.render(outfile)
generated_code = outfile.getvalue()

# Step 5: Output the result.
# The `generated_code` variable now holds the complete Python module text.
print(generated_code)

# In a real script, this could be written to a file:
# with open("models.py", "w") as f:
#     f.write(generated_code)

This script demonstrates a powerful architectural pattern. The entire workflow is executed within a single Python process, with no external dependencies beyond the required libraries (sqlalchemy, sqlacodegen). It is portable, blazingly fast, and completely deterministic. This self-contained, in-process nature is what allows it to achieve the same level of seamless integration and performance that developers expect from tools like OpenAPI client generators, directly addressing the core requirements of the initial query.IV. Implementation Strategies and Ecosystem AnalysisWith the core workflow established, it is crucial to situate this approach within the broader ecosystem of code generation tools and provide actionable strategies for integrating it into a professional development lifecycle. A direct comparison with alternative methodologies will solidify the rationale behind the recommended solution.A Comparative Analysis of Generation MethodologiesThe decision of which tool to use for code generation depends heavily on the specific requirements of the task. For generating sqlmodel classes from SQL DDL, the trade-offs between different approaches are stark.Table 2: Comparison of Model Generation ApproachesCriterionsqlacodegen + In-Memory DB (Recommended)LLM-Based Generationdatamodel-code-generatorPrimary Input SourceSQL DDL (via in-memory materialization)SQL DDL, Natural Language PromptsOpenAPI, JSON Schema, GraphQL 9SpeedExtremely Fast (milliseconds, in-process)Slow (seconds, network-dependent)Very Fast (milliseconds, in-process)Determinism100% DeterministicNon-Deterministic (Probabilistic)100% DeterministicReliabilityHigh; based on formal schema reflectionVariable; prone to subtle errors/hallucinationsHigh; based on formal specificationsConfigurationHigh (via command-line options)Moderate (via prompt engineering)High (via command-line options)Ideal Use CaseProduction-grade, automated generation of ORM models from a canonical SQL schema.Rapid prototyping, one-off conversions, or creative scaffolding where precision is not paramount.Generating Pydantic models from data interchange formats and API specifications.This comparison clearly illustrates why the sqlacodegen workflow is the superior choice for this specific problem. It combines the determinism and speed of a dedicated tool with the correct input source, whereas LLMs fail on determinism and datamodel-code-generator is designed for a different problem domain.Addressing the Alternative: Why Not datamodel-code-generator?datamodel-code-generator is an excellent and highly popular tool within the Python ecosystem for generating Pydantic models.9 However, it is fundamentally the wrong tool for this task. Its design purpose is to create data models from sources that define data structures for serialization and validation, such as OpenAPI, JSON Schema, and GraphQL schemas.10 It has no built-in capability to parse or understand SQL DDL.While it might be theoretically possible to create a multi-step process that converts SQL DDL to an intermediate format like JSON Schema and then feeds that to datamodel-code-generator, this would be a brittle, complex, and custom-built solution. It would require writing and maintaining a separate parser, effectively re-implementing the functionality that sqlacodegen already provides in a robust and mature package. Discussions in the community confirm that generating SQLModel classes with table metadata and relationships from this tool requires significant custom post-processing and is not a supported feature.11 Therefore, sqlacodegen remains the direct, purpose-built solution.Integration into the Development LifecycleThe true power of the scripted, in-memory generation workflow is realized when it is integrated into standard development practices. This transforms model generation from a manual chore into a seamless, automated part of the development lifecycle.As a Command-Line Tool: The Python script from Section III can be easily enhanced with a library like argparse or typer to create a standalone CLI tool. This would allow developers to run a simple command like python generate_models.py --input-ddl schema.sql --output-file app/models.py.In CI/CD Pipelines: The generation script can be executed as a step in a continuous integration pipeline. This can serve as a validation check, ensuring that any committed changes to the canonical .sql schema file are reflected in the application models. The build can fail if the generated models differ from the committed versions, enforcing synchronization.With Git Hooks: For the tightest feedback loop, the script can be configured to run as a pre-commit hook. When a developer modifies and stages the schema.sql file, the hook automatically runs, regenerates the models.py file, and adds it to the commit. This ensures that the models are always perfectly in sync with the schema definition before the code is even pushed.With Task Runners: The command can be aliased in a project's task runner, such as a Makefile or in the [tool.poetry.scripts] section of a pyproject.toml file. This provides a simple, memorable command (e.g., make models or poetry run models) for developers to invoke the generation process manually when needed.V. Conclusion: Adopting a Production-Ready Generation StrategyThe challenge of generating sqlmodel classes from SQL DDL highlights a critical decision point in modern software engineering: choosing between the flexible but unpredictable power of probabilistic tools like LLMs and the structured, reliable precision of deterministic engineering tools. While LLMs offer a compelling solution for rapid prototyping, they are fundamentally unsuited for the automated generation of foundational application code where correctness and consistency are paramount.Recap of the SolutionThis report has detailed a robust, high-performance, and fully deterministic workflow that serves as a production-ready alternative. The solution is a two-stage process executed within a single, self-contained Python script:Materialization: A transient, in-memory SQLite database is created to execute the source SQL DDL, transforming it from static text into a live, queryable schema.Reflection: The sqlacodegen library, using its purpose-built sqlmodels generator, connects to this ephemeral database, introspects its complete structure via SQLAlchemy, and generates clean, idiomatic sqlmodel code.Revisiting the "Why": Beyond Speed to Engineering RigorThe decision to adopt this workflow is driven by more than just performance. It represents a commitment to engineering rigor. By replacing a non-deterministic, network-bound process with a local, deterministic, and transparent one, development teams gain several critical advantages:Reliability: The output is guaranteed to be consistent, eliminating a potential source of subtle and difficult-to-diagnose bugs.Automation: The process is fast and simple enough to be fully automated and integrated into CI/CD pipelines and pre-commit hooks, enforcing consistency across the entire team.Maintainability: The data models, a critical layer of the application, are tied directly and verifiably to a single source of truth—the SQL DDL.Final EndorsementFor any professional development environment where sqlmodel is used and the database schema is defined via SQL DDL, the in-memory materialization and reflection workflow is the industry-standard, expert-recommended solution. It provides the speed and convenience of modern code generators while upholding the essential principles of deterministic, reliable, and maintainable software construction.
